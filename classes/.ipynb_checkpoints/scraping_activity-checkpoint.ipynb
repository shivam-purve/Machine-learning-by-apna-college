{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e879534b-2e6b-45f9-9a48-046795f7ec55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data from page 1\n",
      "Downloaded data from page 2\n",
      "Downloaded data from page 3\n",
      "Downloaded data from page 4\n",
      "Downloaded data from page 5\n",
      "Downloaded data from page 6\n",
      "Downloaded data from page 7\n",
      "Downloaded data from page 8\n",
      "Downloaded data from page 9\n",
      "Downloaded data from page 10\n",
      "No valid page anymore...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"scrapped_data\", exist_ok=True)\n",
    "\n",
    "page_count = 1\n",
    "\n",
    "while True:\n",
    "    URL = f\"https://quotes.toscrape.com/page/{page_count}/\"\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(URL)\n",
    "        res.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request failed:\", e)\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    quotes = soup.select(\"div.quote\")\n",
    "\n",
    "    if not quotes:\n",
    "        print(\"No valid page anymore...\")\n",
    "        break\n",
    "\n",
    "    with open(f\"scrapped_data/quotes{page_count}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(res.text)\n",
    "        print(f\"Downloaded data from page {page_count}\")\n",
    "\n",
    "    page_count += 1\n",
    "    time.sleep(1)  # polite delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be65b0c9-bbf1-411f-b1d6-6f632641e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract useful info\n",
    "page_count = 1\n",
    "life_quotes = []\n",
    "\n",
    "while(page_count<=10):\n",
    "    with open(f\"scrapped_data/quotes{page_count}.html\",\"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content,\"lxml\")\n",
    "        all_quotes = soup.select(\"div.quote\")\n",
    "        for q in all_quotes:\n",
    "            all_tags = []\n",
    "            for tag in q.select(\".tags .tag\"):\n",
    "                all_tags.append(tag.get_text())\n",
    "            if \"life\" in all_tags:\n",
    "                text = q.select_one(\"span.text\").get_text()\n",
    "                author = q.select_one(\"small.author\").get_text()\n",
    "                life_quotes.append([text,author])\n",
    "    page_count += 1\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(life_quotes,columns = [\"quote\",\"author_name\"])\n",
    "df.to_csv(\"cleaned_data/quote.csv\" , index = False)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804fb4b-c812-4a96-817f-edfa33386bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
